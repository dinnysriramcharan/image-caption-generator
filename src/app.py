import streamlit as st
from PIL import Image, ImageDraw
import requests
from io import BytesIO
import random

st.set_page_config(page_title="Image Caption Generator", layout="centered")
st.title("ðŸ–¼ï¸ AI-Powered Image Caption Generator")
st.write("Upload an image and receive a descriptive caption generated by AI.")
st.write("This demo shows how the system would work with state-of-the-art vision-language models.")

# Show info about model loading
st.info("ðŸ”§ **Demo Mode**: This is a demonstration version. In production, this would load a pre-trained model like BLIP, GIT, or similar vision-language models from Hugging Face.")

@st.cache_resource
def load_model():
    """Mock model loading for demonstration"""
    return "demo_model_loaded"

# Load model
with st.spinner("Loading AI model... This may take a moment on first run."):
    model_status = load_model()

def analyze_image_content(image):
    """Analyze image to create contextual captions (demo version)"""
    # Convert to RGB if needed
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # Get basic image info
    width, height = image.size
    
    # Simple analysis based on image characteristics
    # This is a demo - real implementation would use computer vision
    aspect_ratio = width / height
    
    # Generate contextual responses based on image properties
    if aspect_ratio > 1.5:
        scene_type = "landscape"
    elif aspect_ratio < 0.7:
        scene_type = "portrait"
    else:
        scene_type = "scene"
    
    # Demo captions that would come from real AI analysis
    sample_captions = {
        "landscape": [
            "a beautiful landscape with natural scenery",
            "an outdoor view showing the natural environment",
            "a wide scenic view of the landscape"
        ],
        "portrait": [
            "a close-up view focusing on the main subject",
            "a detailed image showing the central focus",
            "a portrait-style image with clear detail"
        ],
        "scene": [
            "an image showing various elements in the scene",
            "a balanced composition with multiple features",
            "a scene capturing different aspects of the moment"
        ]
    }
    
    return random.choice(sample_captions[scene_type])

def generate_caption(image):
    """Generate caption for the uploaded image"""
    try:
        # In production, this would use the loaded vision-language model
        # For demo, we'll use basic image analysis
        caption = analyze_image_content(image)
        
        # Add note about demo mode
        return f"{caption} (Generated using demo logic - real AI would provide detailed analysis)"
        
    except Exception as e:
        return f"Error analyzing image: {str(e)}"

def load_sample_image():
    """Load a sample image for demonstration"""
    try:
        # Using a sample image from the internet
        url = "https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg"
        response = requests.get(url, timeout=10)
        image = Image.open(BytesIO(response.content))
        return image
    except:
        # Create a simple colored image if we can't load from internet
        img = Image.new('RGB', (300, 200), color=(100, 150, 200))
        draw = ImageDraw.Draw(img)
        draw.text((50, 80), "Sample Image", fill=(255, 255, 255))
        return img

# Create two columns for layout
col1, col2 = st.columns([2, 1])

with col2:
    st.subheader("Options")
    
    # Add sample image option
    if st.button("Try Sample Image ðŸ±"):
        sample_image = load_sample_image()
        if sample_image:
            st.session_state.sample_image = sample_image
        else:
            st.warning("Could not load sample image. Please upload your own image.")

with col1:
    uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

# Handle image display and processing
image_to_process = None

if uploaded_file:
    image_to_process = Image.open(uploaded_file)
    st.image(image_to_process, caption="Uploaded Image", use_container_width=True)
elif 'sample_image' in st.session_state:
    image_to_process = st.session_state.sample_image
    st.image(image_to_process, caption="Sample Image", use_container_width=True)

if image_to_process:
    if st.button("Generate Caption", type="primary"):
        with st.spinner("Analyzing image... This may take a few seconds."):
            caption = generate_caption(image_to_process)
        st.success(f"**Generated Caption:** {caption}")
        
        # Add some additional information
        st.info("ðŸ’¡ **How it works:** This AI model analyzes visual features in your image and generates natural language descriptions using advanced computer vision and natural language processing.")

st.markdown("---")

# Add instructions and information
st.markdown("### ðŸ“‹ Instructions")
st.markdown("""
1. **Upload an image** using the file uploader above (JPG, JPEG, or PNG)
2. **Or try the sample image** to see how it works
3. **Click 'Generate Caption'** to get an AI-generated description
4. The model will analyze the image and provide a descriptive caption
""")

st.markdown("### ðŸ¤– About the AI Models")
st.markdown("""
- **Production Models:** BLIP, GIT, LLaVA, or GPT-4V for image captioning
- **Developers:** Salesforce Research, Microsoft Research, OpenAI
- **Capabilities:** State-of-the-art image understanding and natural language generation
- **Architecture:** Vision Transformer + Large Language Models
""")

st.markdown("### ðŸš€ Technical Implementation")
with st.expander("Click to see technical information"):
    st.markdown("""
    **Production Architecture:**
    - **Vision Encoder:** Processes images to extract visual features (ViT, ResNet, etc.)
    - **Language Decoder:** Generates natural language descriptions (Transformer, GPT, etc.)
    - **Training:** Pre-trained on millions of image-text pairs
    
    **Processing Pipeline:**
    1. Image preprocessing and normalization
    2. Feature extraction using vision transformer
    3. Caption generation using beam search or sampling
    4. Post-processing for natural language output
    
    **Demo Implementation:**
    - Shows the user interface and workflow
    - Demonstrates image upload and processing
    - Provides example of how AI captions would be generated
    - Ready for integration with real vision-language models
    """)

# Add implementation guide
st.markdown("### ðŸ’» Implementation Guide")
with st.expander("How to add real AI models"):
    st.markdown("""
    **To enable real AI captioning:**
    
    1. **Install dependencies:**
    ```bash
    pip install transformers torch pillow
    ```
    
    2. **Replace the demo function with real model:**
    ```python
    from transformers import BlipProcessor, BlipForConditionalGeneration
    
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    
    def generate_caption(image):
        inputs = processor(image, return_tensors="pt")
        out = model.generate(**inputs)
        return processor.decode(out[0], skip_special_tokens=True)
    ```
    
    3. **Alternative models to try:**
    - `microsoft/git-base-coco` (GIT model)
    - `Salesforce/blip-image-captioning-large` (BLIP large)
    - `nlpconnect/vit-gpt2-image-captioning` (ViT-GPT2)
    """)

# Footer
st.markdown("---")
st.markdown("**Note:** This is a demonstration of an AI-powered image captioning system. In production, this would use state-of-the-art vision-language models. The interface and workflow are ready for real AI integration.")