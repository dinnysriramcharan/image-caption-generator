{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption Generator - Experimentation Notebook\n",
    "\n",
    "This notebook demonstrates the core functionality of the image caption generator and provides a foundation for experimentation and model improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLIP model for image captioning\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url):\n",
    "    \"\"\"Load an image from a URL\"\"\"\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    return image.convert('RGB')\n",
    "\n",
    "def generate_caption(image, max_length=50, num_beams=5):\n",
    "    \"\"\"Generate caption for an image\"\"\"\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_length=max_length, num_beams=num_beams)\n",
    "    \n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def display_image_with_caption(image, caption):\n",
    "    \"\"\"Display an image with its generated caption\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Caption: {caption}\", fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample image\n",
    "sample_urls = [\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Collage_of_Nine_Dogs.jpg/1200px-Collage_of_Nine_Dogs.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Culinary_fruits_front_view.jpg/1200px-Culinary_fruits_front_view.jpg\"\n",
    "]\n",
    "\n",
    "for i, url in enumerate(sample_urls):\n",
    "    try:\n",
    "        print(f\"\\n--- Sample Image {i+1} ---\")\n",
    "        image = load_image_from_url(url)\n",
    "        caption = generate_caption(image)\n",
    "        print(f\"Generated Caption: {caption}\")\n",
    "        display_image_with_caption(image, caption)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {i+1}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Different Generation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a test image\n",
    "test_image = load_image_from_url(sample_urls[0])\n",
    "\n",
    "# Test different parameters\n",
    "parameters = [\n",
    "    {\"max_length\": 30, \"num_beams\": 3},\n",
    "    {\"max_length\": 50, \"num_beams\": 5},\n",
    "    {\"max_length\": 100, \"num_beams\": 8}\n",
    "]\n",
    "\n",
    "print(\"Comparing different generation parameters:\\n\")\n",
    "for i, params in enumerate(parameters):\n",
    "    caption = generate_caption(test_image, **params)\n",
    "    print(f\"Config {i+1} (max_length={params['max_length']}, num_beams={params['num_beams']}):\")\n",
    "    print(f\"  Caption: {caption}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Information and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model information\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Processor information\n",
    "print(\"\\nProcessor Configuration:\")\n",
    "print(f\"Image processor: {type(processor.image_processor).__name__}\")\n",
    "print(f\"Tokenizer: {type(processor.tokenizer).__name__}\")\n",
    "print(f\"Vocabulary size: {processor.tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark inference time\n",
    "test_image = load_image_from_url(sample_urls[0])\n",
    "num_runs = 5\n",
    "\n",
    "print(f\"Benchmarking inference time over {num_runs} runs...\\n\")\n",
    "\n",
    "times = []\n",
    "for i in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    caption = generate_caption(test_image)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    inference_time = end_time - start_time\n",
    "    times.append(inference_time)\n",
    "    print(f\"Run {i+1}: {inference_time:.3f}s - Caption: {caption}\")\n",
    "\n",
    "avg_time = sum(times) / len(times)\n",
    "print(f\"\\nAverage inference time: {avg_time:.3f}s\")\n",
    "print(f\"Min time: {min(times):.3f}s\")\n",
    "print(f\"Max time: {max(times):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Experiments\n",
    "\n",
    "This notebook provides a foundation for further experimentation:\n",
    "\n",
    "1. **Fine-tuning**: Train on domain-specific datasets\n",
    "2. **Evaluation**: Implement BLEU, CIDEr, and other metrics\n",
    "3. **Comparison**: Test different vision-language models\n",
    "4. **Optimization**: Explore quantization and acceleration techniques\n",
    "5. **Analysis**: Study model attention patterns and failure cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}